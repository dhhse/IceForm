{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Создадим словарь морфологических обозначений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        xml = f.read()\n",
    "        soup = BeautifulSoup(xml, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = open_file('Формулы/xml/sagHdr.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_POS = {}\n",
    "\n",
    "for i in soup.find_all(\"list\")[27]:\n",
    "    if i != '\\n':\n",
    "        CODE_POS[i.attrs['id']] = i.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определяем часть речи по коду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_short = {\n",
    "    'unspecified': 'UNK',\n",
    "    'numeral': 'NUM',\n",
    "    'verb': 'V',\n",
    "    'noun': 'N',\n",
    "    'adjective': 'ADJ',\n",
    "    'article': 'ART',\n",
    "    'pronoun': 'PRO',\n",
    "    'foreign-word': 'F',\n",
    "    'conjunction': 'CONJ',\n",
    "    'comparative-degree': 'COMP',\n",
    "    'superlative-degree': 'SLAT',\n",
    "    'preposition': 'PREP',\n",
    "    'exclamation': 'EX',\n",
    "    'abbreviation': 'ABB',\n",
    "    'adverb': 'ADV',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS(code, CODE_POS):\n",
    "    '''\n",
    "    Функция выдает часть речи по коду\n",
    "    '''\n",
    "    \n",
    "    pos = CODE_POS.get(code)\n",
    "    if pos:\n",
    "        pos = pos.split(' ')[0]\n",
    "        pos = pos_short[pos]\n",
    "    \n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PREP'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS('ao', CODE_POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Открываем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Формулы/xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_lemmas_from_xml(soup, word_lem_morph):\n",
    "\n",
    "    data, data_lemmas, data_morph, data_all = [], [], [], []\n",
    "\n",
    "    for sent in soup.find_all(\"s\"):\n",
    "    \n",
    "        s1, s2, s3, s4 = [], [], [], []\n",
    "    \n",
    "        for wo in sent:\n",
    "        \n",
    "            if wo.name == 'w':\n",
    "                x = wo.attrs['type']\n",
    "                y = wo.attrs['lemma'].lower()\n",
    "                z = wo.text.lower()\n",
    "                w = POS(x, CODE_POS)\n",
    "                s1.append(z)\n",
    "                s2.append(y)\n",
    "                s3.append(x)\n",
    "                s4.append(y + '_' + w)\n",
    "                s4.append(z + '_' + w)\n",
    "                \n",
    "               # if (y, x) not in word_lem_morph[z]:\n",
    "               #     word_lem_morph[z].append((y, x))\n",
    "       \n",
    "            if wo.name == 'c':\n",
    "                s1.append(wo.text)\n",
    "                s2.append(wo.text)\n",
    "                s3.append(wo.text)\n",
    "                s4.append(wo.text)\n",
    "        \n",
    "        data.append(s1)\n",
    "        data_lemmas.append(s2)\n",
    "        data_morph.append(s3)\n",
    "        data_all.append(s4)\n",
    "    \n",
    "    return data, data_lemmas, data_morph, data_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "all_xmls = []\n",
    "all_words = []\n",
    "all_lemmas = []\n",
    "all_morphs = []\n",
    "all_data = []\n",
    "word_lem_morph = defaultdict(list)\n",
    "\n",
    "\n",
    "for file_name in os.listdir(path):\n",
    "    \n",
    "    if file_name not in ('.DS_Store', 'sagHdr.xml'):\n",
    "        \n",
    "        p = path + '/' + file_name\n",
    "        soup = open_file(p)\n",
    "        data, data_lemmas, data_morph, data_all = get_lemmas_from_xml(soup, word_lem_morph)\n",
    "        all_xmls.append([data, data_lemmas, data_morph])\n",
    "        \n",
    "        all_w = [l for i in data for l in i]\n",
    "        all_w_lem = [l for i in data_lemmas for l in i]\n",
    "        all_w_mor = [l for i in data_morph for l in i]\n",
    "        all_ = [l for i in data_all for l in i]\n",
    "        \n",
    "        #print(file_name,': ', str(len(all_w)))\n",
    "        \n",
    "        all_words += all_w\n",
    "        all_lemmas += all_w_lem\n",
    "        all_morphs += all_w_mor\n",
    "        all_data += all_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPOUNDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ = all_words + all_lemmas\n",
    "num = len(all_)\n",
    "\n",
    "count = Counter(all_)\n",
    "for word in count:\n",
    "    count[word] = count[word] / num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = len(all_data)\n",
    "\n",
    "count_pos = Counter(all_data)\n",
    "for word in count_pos:\n",
    "    count_pos[word] = count_pos[word] / num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ófeigur_N',\n",
       " 'heita_V',\n",
       " 'hét_V',\n",
       " 'maður_N',\n",
       " '.',\n",
       " 'hann_PRO',\n",
       " 'búa_V',\n",
       " 'bjó_V',\n",
       " 'norður_ADV',\n",
       " 'í_PREP']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(count_pos.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ófeigur', 'hét', 'maður', '.', 'hann', 'bjó', 'norður', 'í', 'miðfirði', 'á']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(count.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0063796127421547"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count['maður']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006730307456802969"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_pos['maður_N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_ngrams(text, num_n, top, all_=False):\n",
    "    \n",
    "    grams = ngrams(text, num_n)\n",
    "    gram_freq = collections.Counter(grams)\n",
    "    \n",
    "    if all_: return gram_freq\n",
    "    return gram_freq.most_common(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_values(beg_ngrams, ng, beg):\n",
    "    val = beg_ngrams.get(ng)\n",
    "    if val is None: val = 0\n",
    "    beg[ng] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compound_part(word, count, end=False):\n",
    "    \n",
    "    length = len(word)\n",
    "    variants = {}\n",
    "\n",
    "    for n in range(3, 20):\n",
    "        \n",
    "        if end:\n",
    "            ind = length - n - 2\n",
    "            if ind < 0: ind = 0\n",
    "            w = word[ind:]\n",
    "        else:\n",
    "            w = word[:n]\n",
    "    \n",
    "        val = count.get(w)\n",
    "        if val is None: val = 0\n",
    "        variants[w] = val\n",
    "    \n",
    "    m = max(variants.values())\n",
    "    return max([i for i in variants if variants[i]==m])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound_splitter(word, n):\n",
    "    \n",
    "    if n == 2:\n",
    "        w2 = compound_part(word, count_pos, end=True)\n",
    "        word = word[:len(word) - len(w2)]\n",
    "        w1 = compound_part(word, count)\n",
    "        ans = [w1, w2[:-2]]\n",
    "        \n",
    "    elif n > 2:\n",
    "\n",
    "        w = compound_part(word, count_pos, end=True)\n",
    "        word = word[:len(word) - len(w)]\n",
    "        ans = []\n",
    "        \n",
    "        if word != '':\n",
    "        \n",
    "            for i in range(n-1):\n",
    "    \n",
    "                w1 = compound_part(word, count)\n",
    "                ans.append(w1)       \n",
    "                word = word[len(w1):]\n",
    "        \n",
    "        ans.append(w[:-2])\n",
    "    \n",
    "    return ans\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ólífis', 'maður']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compound_splitter('ólífismaður_N', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ólífisdóttir\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ólífis', 'dóttir', 'maður']"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compound_splitter('ólífisdóttirmaður_N', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['höskulds', 'dóttir']"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compound_splitter('höskuldsdóttir_N', 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['мама', 'dóttir']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compound_splitter('мамаdóttir_N', 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['langskip', 'a', 'maður']"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compound_splitter('langskipamaður_N', 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
