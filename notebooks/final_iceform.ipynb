{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формульные конструкции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Цель проекта - автоматизированный (непредзаданный) поиск повторяющихся конструкций с той или иной степенью вариативности.\n",
    "* Данные - Корпус древнеисландских саг\n",
    "* Объем корпуса - 1.5 млн токенов\n",
    "* Разметка - словоизменительный тип и лемма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа состоит из следующих этапов:\n",
    "\n",
    "    1) Формирование списка нграмм\n",
    "    2) Фильтрация на основе лингвистических особенностей исландского языка\n",
    "    3) «Схлопывание» контекстных вариантов\n",
    "    4) Кластеризация\n",
    "    5) Создание базы данных\n",
    "    6) Создание сайта\n",
    "\n",
    "В настоящем ноутбуке представлен код для реализации 1-5 этапов проекта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "from collections import Counter, defaultdict, namedtuple\n",
    "import sqlite3\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import pickle as pkl\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.util import ngrams \n",
    "\n",
    "from scipy.cluster.hierarchy import complete, dendrogram, fcluster\n",
    "from gensim.models.fasttext import FastText, load_facebook_vectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect(\"iceform.db\")\n",
    "cur = db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Планирование и архитектура"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Глобальные переменные:\n",
    "    \n",
    "- настройки языковые - json-файл\n",
    "- путь к файлам\n",
    "- папка для пиклов\n",
    "\n",
    "\n",
    "Классы:\n",
    "\n",
    "- Node Word- слова\n",
    "- Ngram: набор слов \n",
    "\n",
    "Словари:\n",
    "- координаты\n",
    "- частотность\n",
    "=> в базу вместе с нграммой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"\n",
    "CREATE TABLE text (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    text_name TEXT,\n",
    "    file_name TEXT\n",
    ")\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE token (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    wordform TEXT,\n",
    "    morph TEXT,\n",
    "    pos TEXT,\n",
    "    lemma TEXT,\n",
    "    ngram_text_id INT\n",
    ")\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE text_content (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    text_id INT,\n",
    "    sentence_id INT,\n",
    "    idx INT,\n",
    "    token_id INT\n",
    ")\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE ngram_item (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    representation TEXT,\n",
    "    lemma TEXT,\n",
    "    pos TEXT,\n",
    "    kind TEXT\n",
    ")\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE ngram_content (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    ngram_id INT,\n",
    "    ngram_item_id INT,\n",
    "    token_id INT,\n",
    "    keep BOOL\n",
    ")\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE ngram_coords (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    unique_ngram_id INT,\n",
    "    text_id INT,\n",
    "    sentence_id INT,\n",
    "    start INT,\n",
    "    end INT,\n",
    "    short_ngram_id INT\n",
    ");\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE short_ngrams (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    n_occurrences INT,\n",
    "    representation INT\n",
    ")\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE clustering (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    ngram_id INT,\n",
    "    short_ngram_id INT,\n",
    "    cluster_idx INT\n",
    ")\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE unique_ngrams (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT, \n",
    "    n_occurrences INT, \n",
    "    short_ngram_id INT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Парсинг корпуса\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Задача:</b> распознавание файлов и запись в базу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Обработка расшифровок морфологических тегов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корпус саг состоит из: \n",
    "   * 49 xml-файлов, содрержащих тексты саг в TEI <br> \n",
    "      Каждое слово в корпусе сопровождается леммой и морфологическим разбором: <br> \n",
    "      Пример: ```<w lemma=\"Þorgerður\" type=\"nven-s\">Þorgerður</w>``` \n",
    "  \n",
    "  \n",
    "  \n",
    "   \n",
    "   * Файл sagHdr.xml, содержащий расшифровку морфологических тегов<br> \n",
    "      Пример: ```<item id=\"nven-s\" occurs=\"5484\">noun feminie singular nominative proper-noun</item>```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства дальнейшей работы из расшифровок морфологических тегов были автоматически выделены наиболее значимые для анализа характеристики и записанные в словарь, содержащий следующие ключи: \n",
    "   * pos - часть речи\n",
    "   * gram - грамматические характеристики слов (на данном этапе учитывается только падеж у существительных и прилагательных)\n",
    "   * dep - характеристики зависимых или главных слов, необходимых для соблюдения правил согласования и подчинения \n",
    "   \n",
    "Далее каждому тегу был сопоставлен соответвующий разбор (CODE_POS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_codes = \"/home/dkbrz/github/IceForm/xml/sagHdr.xml\"\n",
    "PATH_texts = \"/home/dkbrz/github/IceForm/xml\"\n",
    "PATH_deps = \"/home/dkbrz/github/IceForm/2020-06-21/dependencies.json\"\n",
    "PATH_gram = \"/home/dkbrz/github/IceForm/2020-06-21/gram_info.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GramInfo:\n",
    "    \n",
    "    \"\"\"\n",
    "    GramInfo is a class that:\n",
    "        1) stores descriptions of morphological tags used for saga annotation\n",
    "        2) stores a system of abbreviations, rules, and weights for easy preprocessing\n",
    "        3) provides methods for preprocessing morphological tags descriptions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 path_deps=\"dependencies.json\", \n",
    "                 path_codes=\"xml/sagHdr.xml\", \n",
    "                 path_gram=\"gram_info.json\"):\n",
    "        self.path_deps = path_deps\n",
    "        self.path_codes = path_codes\n",
    "        self.path_gram = path_gram\n",
    "        self.depend = self.add_depend()\n",
    "        self.morph_info = self.add_morph_info()\n",
    "        self.add_gram_info()\n",
    "    \n",
    "    def add_morph_info(self):\n",
    "        \"\"\"\n",
    "        sagHdr.xml contains morphological tags and their descriptions\n",
    "        \"\"\"\n",
    "        morph_info = self.open_xml(self.path_codes)\n",
    "        morph_info = morph_info.find_all(\"list\")[27]\n",
    "        morph_info = [i for i in morph_info if i != '\\n']\n",
    "        return morph_info\n",
    "        \n",
    "    def add_depend(self):\n",
    "        \"\"\"\n",
    "        dependencies.json contains morphological dependencies\n",
    "        \"\"\"\n",
    "        return self.open_json(self.path_deps)\n",
    "        \n",
    "    def add_gram_info(self):\n",
    "        \"\"\"\n",
    "        gram_info.json contains morphological inforamtion \n",
    "        Categories:\n",
    "            POS_short - short name of pos\n",
    "            POS_values - weight of pos\n",
    "            gram_short - short name of certain grammatical information\n",
    "            case_short - short name of case\n",
    "            case_values - weight of case\n",
    "        \"\"\"\n",
    "        gram_info = self.open_json(self.path_gram)\n",
    "        self.POS_short = gram_info['POS_short']\n",
    "        self.POS_values = gram_info['POS_values']\n",
    "        self.gram_short = gram_info['gram_short']\n",
    "        self.case_short = gram_info['case_short']\n",
    "        self.case_values = gram_info['case_values']\n",
    "        \n",
    "    def get_value(self, morph, attr):\n",
    "        \"\"\"\n",
    "        This function finds abbreviations for morphological tags\n",
    "        \"\"\"\n",
    "        for name in morph:\n",
    "            val = self.__dict__[attr].get(name)\n",
    "            if val: return val\n",
    "        return None\n",
    "    \n",
    "    def set_dependencies(self, pos, name, morph_dict):\n",
    "        \"\"\"\n",
    "        This function sets dependencies based on a morphological tag\n",
    "        \"\"\"\n",
    "        if pos == 'CINF': dep = 'Inf'\n",
    "        elif pos == 'Inf': dep = 'CINF'\n",
    "        else: dep = self.get_value(morph, 'depend')\n",
    "        if dep: morph_dict['dep'] = dep   \n",
    "    \n",
    "    def set_case(self, morph, morph_dict):\n",
    "        \"\"\"\n",
    "        This function sets case based on a morphological tag\n",
    "        \"\"\"\n",
    "        case = self.get_value(morph, 'case_short')\n",
    "        if case: morph_dict['gram'] = case\n",
    "         \n",
    "    def set_pos(self, morph, morph_dict):\n",
    "        \"\"\"\n",
    "        This function sets POS based on a morphological tag\n",
    "        \"\"\"\n",
    "        pos = self.POS_short.get(morph[0])\n",
    "        if pos in ('V', 'PRO', 'C', 'N') and len(morph) > 1:\n",
    "            alter_pos = self.get_value(morph, 'gram_short')\n",
    "            if alter_pos: pos = alter_pos\n",
    "        morph_dict['pos'] = pos \n",
    "        return pos\n",
    "    \n",
    "    @staticmethod\n",
    "    def open_json(path):\n",
    "        \"\"\"\n",
    "        This function opens json file and\n",
    "        converts it to BeautifulSoup for an easy data extraction process \n",
    "        \"\"\"\n",
    "        with open(path) as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def open_xml(path):\n",
    "        \"\"\"\n",
    "        This function opens xml file \n",
    "        \"\"\"\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            xml = f.read()\n",
    "            data = BeautifulSoup(xml, 'lxml')\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grinfo = GramInfo(path_codes=PATH_codes, path_deps=PATH_deps, path_gram=PATH_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_POS = defaultdict(dict)\n",
    "\n",
    "deps_pos = {'PREP', 'ART', 'CINF', 'Adj', 'Inf', 'PRO'}\n",
    "case_pos = {'N', 'Adj', 'PPro', 'Prp'}\n",
    "\n",
    "for morph_line in grinfo.morph_info:\n",
    "    \n",
    "    id_ = morph_line.attrs['id']\n",
    "    morph = morph_line.string.split(' ')\n",
    "    pos = grinfo.set_pos(morph, CODE_POS[id_])\n",
    "    \n",
    "    if pos in deps_pos:\n",
    "        grinfo.set_dependencies(pos, morph, CODE_POS[id_])\n",
    "    \n",
    "    if pos in case_pos:\n",
    "        grinfo.set_case(morph, CODE_POS[id_])\n",
    "\n",
    "CODE_POS['pun']['pos'] = 'pun'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Парсинг текстов саг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый текст в корпусе был распаршен и записан в базу. По мере обработки тектов саг, для каждого токена была сохранена информация о его лемме, словоформе и морфологической информации, полученной на предудщем этапе. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "    \n",
    "    def __init__(self, wordform, morph, pos, lemma):\n",
    "        self.idx = None\n",
    "        self.wordform = wordform\n",
    "        self.morph = morph\n",
    "        self.pos = pos\n",
    "        self.lemma = lemma\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if other.wordform != self.wordform:\n",
    "            return False\n",
    "        if other.morph != self.morph:\n",
    "            return False \n",
    "        return True\n",
    "\n",
    "TextContent = namedtuple(\"TextContent\", [\"text_id\", \"sentence_id\", \"w_idx\", \"token_id\"])\n",
    "\n",
    "class TokenStorage:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tdict = {}\n",
    "        self.tlist = []\n",
    "        self.tidx = 0\n",
    "        \n",
    "    def get_id(self, wordform, morph):\n",
    "        pair = (wordform, morph)\n",
    "        if pair in self.tdict:\n",
    "            return self.tdict[pair], True\n",
    "        else:\n",
    "            self.tdict[pair] = self.tidx\n",
    "            self.tidx += 1\n",
    "            return self.tdict[pair], False\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        token.idx = self.tdict[(token.wordform, token.morph)] \n",
    "        self.tlist.append(token)\n",
    "        \n",
    "    def to_db(self):\n",
    "        start = min(self.tdict.values())\n",
    "        assert start == 0\n",
    "        data = [(t.idx, t.wordform, t.morph, t.pos, t.lemma) for t in self.tlist]\n",
    "        cur.executemany(\"\"\"\n",
    "        INSERT INTO token (\n",
    "            id, wordform, morph, pos, lemma\n",
    "        ) VALUES (?, ?, ?, ?, ?)\"\"\", data)\n",
    "        db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text:\n",
    "    \n",
    "    def __init__(self, idx, fname):\n",
    "        self.idx = idx\n",
    "        self.fname = fname\n",
    "        self.soup = self.read_file()\n",
    "        self.title = self.find_title()\n",
    "        self.text_content = self.parse_text()\n",
    "        \n",
    "    def read_file(self):\n",
    "        path = os.path.join(PATH_texts, self.fname)\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            soup = BeautifulSoup(f.read(), 'lxml')\n",
    "        return soup\n",
    "    \n",
    "    def find_title(self):\n",
    "        return self.soup.find(\"title\").get_text()\n",
    "    \n",
    "    def parse_text(self):\n",
    "        global t_storage\n",
    "        \n",
    "        text_content = []\n",
    "        \n",
    "        for s_id, sentence in enumerate(self.soup.find_all(\"s\")):\n",
    "            w_idx = 0\n",
    "            for word in sentence:\n",
    "                is_word = word.name == 'w' and word.attrs['lemma'].lower() != 'at'\n",
    "                is_punct = word.name == 'c' and s_id != (len(sentence) - 2)                \n",
    "                if not (is_word or is_punct):\n",
    "                    continue\n",
    "                wordform = word.text.lower()\n",
    "                if is_word:\n",
    "                    gram = word.attrs['type']\n",
    "                else:\n",
    "                    gram = 'pun'\n",
    "                idx, known = t_storage.get_id(wordform, gram)\n",
    "                if not known:\n",
    "                    pos = CODE_POS.get(gram, {}).get(\"pos\")\n",
    "                    token = Token(wordform=wordform, morph=gram, pos=pos, lemma=wordform)\n",
    "                    t_storage.add_token(token)\n",
    "                \n",
    "                t_word = TextContent(text_id=self.idx, sentence_id=s_id, w_idx=w_idx, token_id=idx)\n",
    "                text_content.append(t_word)\n",
    "                w_idx += 1\n",
    "        return text_content\n",
    "    \n",
    "    def to_db(self):\n",
    "        cur.execute(\n",
    "            \"INSERT INTO text VALUES (?, ?, ?)\", \n",
    "            (self.idx, self.title, self.fname))\n",
    "        cur.executemany(\"\"\"\n",
    "        INSERT INTO text_content (\n",
    "            text_id, sentence_id, idx, token_id\n",
    "        ) VALUES (?, ?, ?, ?)\"\"\", self.text_content)\n",
    "        db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_storage = TokenStorage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = sorted([\n",
    "    f for f in os.listdir(PATH_texts) \n",
    "    if not f.startswith(\"sagHdr\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=49.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx, fname in tqdm(enumerate(text_list), leave=False, total=len(text_list)):\n",
    "    text = Text(idx, fname)\n",
    "    text.to_db()\n",
    "\n",
    "t_storage.to_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Более абстрактные представления"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В связи с особенностями исландского языка (омонимия, словоизменение, отсутвие фиксированного порядка слов), работать только с леммами слов затруднительно, так как на этапе \"схлопывания\" нграмм есть риск объединить разные по смыслу конструкции. Поэтому слова в корпусе были представленны в следующих видах в зависимости от частеричной принадлежности и ее значимости в формировании смысла конструкции:\n",
    "\n",
    "   * <b>Часть речи</b> — для артиклей, междометий, местоимений и неизвестных слов \n",
    "   * <b>Часть речи + Падеж</b> — для имен собственных и прилагательных\n",
    "   * <b>Часть речи + Падеж + \"_\" + Лемма</b> — для существительных\n",
    "   * <b>Часть речи + \"_\" + Лемма</b> — для глаголов, наречий, союзов и предлогов   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_skip = {'ART', 'Adj', 'EX', 'UNK' , 'PPro', 'PRO', 'Prp'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_representation(morph, pos, lemma):\n",
    "    if pos is None:\n",
    "        pos = \"\"\n",
    "    gram = CODE_POS.get(morph, {}).get('gram', \"\")\n",
    "    ginf = pos + gram\n",
    "    if pos in pos_skip: \n",
    "        return ginf, pos, None, ginf\n",
    "    return ginf, pos, lemma, f\"{ginf}_{lemma}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT id, morph, pos, lemma FROM token\")\n",
    "tokens = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_items_dict = {}\n",
    "ngram_items_list = []\n",
    "token_update = []\n",
    "ngram_items_idx = 0\n",
    "\n",
    "for idx, morph, pos, lemma in tokens:\n",
    "    ginf, pos, lemma, rprs =  get_representation(morph, pos, lemma)\n",
    "    if rprs in ngram_items_dict:\n",
    "        token_update.append((ngram_items_dict[rprs], idx))\n",
    "    else:\n",
    "        ngram_items_dict[rprs] = ngram_items_idx\n",
    "        token_update.append((ngram_items_dict[rprs], idx))\n",
    "        ngram_items_list.append((ngram_items_dict[rprs], pos, lemma, rprs, ginf))\n",
    "        ngram_items_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.executemany(\"INSERT INTO ngram_item (id, pos, lemma, representation, kind) VALUES (?, ?, ?, ?, ?)\", ngram_items_list)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.executemany(\"UPDATE token SET ngram_text_id = ? WHERE id = ?\", token_update)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Формирование списка нграмм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Задача</b>: формирование списка нграмм на основе корпуса "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Формирование списка предложений корпуса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства процесса формирования нграмм, был составлен список предложений всего корпуса, в котором каждое слово сопровождается не только грамматической информацией, но и его расположение в тексте (координаты). Сохранение координат необходимо для осуществления поиска нграмм в текстах, которое будет реализовано на сайте. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NgramCoord = namedtuple(\"NgramCoord\", [\"text_index\", \"sent_index\", \"word_start\", \"word_end\"])\n",
    "\n",
    "def to_ngram_coord(words):\n",
    "    text_index = min([i.text_index for i in words])\n",
    "    sent_index = min([i.sent_index for i in words])\n",
    "    word_start = min([i.word_index for i in words])\n",
    "    word_end = max([i.word_index for i in words])\n",
    "    return NgramCoord(text_index, sent_index, word_start, word_end)\n",
    "                \n",
    "\n",
    "class AllNgrams:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.freq = defaultdict(int)\n",
    "        self.coord = defaultdict(set)\n",
    "    \n",
    "    def add(self, words):\n",
    "        coords = to_ngram_coord(words)\n",
    "        self.coord[words].add(coords)\n",
    "        self.freq[words] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    \n",
    "    def __init__(self, lemma, morph, show, text_index, sent_index, word_index):\n",
    "        self.text_index = text_index\n",
    "        self.sent_index = sent_index\n",
    "        self.word_index = word_index\n",
    "        self.unify(morph)\n",
    "        self.show = show\n",
    "        self.lemma = lemma\n",
    "        \n",
    "    def unify(self, gram):\n",
    "        info = CODE_POS.get(gram)\n",
    "        self.pos = info['pos']\n",
    "        self.if_gram(info)\n",
    "        self.dep = info.get('dep')\n",
    "        \n",
    "    def if_gram(self, info):\n",
    "        if 'gram' in info: self.gram = info['gram']\n",
    "        else: self.gram = []\n",
    "    \n",
    "    def __str__(self):\n",
    "         return self.show\n",
    "        \n",
    "    def __repr__(self):\n",
    "         return self.show\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.show)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.show == other.show\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        return self.show < other.show\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        return self.show > other.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"\n",
    "SELECT token.lemma, morph, representation, text_id, sentence_id, idx \n",
    "FROM text_content\n",
    "    JOIN token ON text_content.token_id = token.id\n",
    "    JOIN ngram_item ON token.ngram_text_id = ngram_item.id\n",
    "\"\"\")\n",
    "sentences = []\n",
    "text, sent = None, None\n",
    "for lemma, morph, show, text_index, sent_index, word_index in cur:\n",
    "    if text_index == text and sent_index == sent:\n",
    "        pass\n",
    "    else:\n",
    "        sentences.append([])\n",
    "        text, sent = text_index, sent_index\n",
    "    sentences[-1].append(Word(lemma, morph, show, text_index, sent_index, word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101359"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всего в корпусе 101359 предложений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle as pkl\n",
    "\n",
    "# with open(\"sentences_lemma.pkl\", \"wb\") as f:\n",
    "#     pkl.dump(sentences, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Формирование нграмм и их фильтрация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формирование нграмм осуществляется с помощью nltk. Возможная длина нграмм составляет от 4 до 8 слов. Каждая нграмма перед добавлением в общий список проверялась на соответвие требованиям, которые были выделены для повторяющихся конструкций в сагах: \n",
    "* обязательность наличия в конструкции глагола\n",
    "* синтаксически цельность (см. README.md)\n",
    "* частеречная значимость не менее 90% (см. README.md)\n",
    "\n",
    "Если нграмма соответвует всем критериям, она добавляется в словари, хранящие их частотности и координаты. В связи с относительно свободным порядком слов в исландском языке, для возможности слопывания конструкций, отличающихся лишь порядом слов, все слова в конструкциях были остортированы по части речи. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ngram_storage:\n",
    "    \"\"\"\n",
    "    Ngram_storage is a class that provides methods for generating and filtering ngrams\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, g=grinfo):\n",
    "        self.g = g\n",
    "        self.reboot()\n",
    "        \n",
    "    def reboot(self):\n",
    "        \"\"\"\n",
    "        This function initializes empty data\n",
    "        \"\"\"\n",
    "        self.pos = defaultdict(int)\n",
    "        self.deps = []\n",
    "        self.gram = []\n",
    "        self.result = []\n",
    "        self.n_words = 0\n",
    "        \n",
    "    def add_gram(self, w):\n",
    "        \"\"\"\n",
    "        This function adds grammatical information\n",
    "        \"\"\"\n",
    "        if w.gram: gr = (w.pos, w.gram)\n",
    "        else: gr = w.pos\n",
    "        self.gram.append(gr)\n",
    "        \n",
    "    def add_word(self, w):\n",
    "        \"\"\"\n",
    "        This function adds word to ngram\n",
    "        \"\"\"\n",
    "        self.result.append(w)\n",
    "        self.pos[w.pos] += 1\n",
    "        self.n_words += 1\n",
    "        self.add_gram(w)\n",
    "        if w.dep: \n",
    "            self.deps.append(w.dep)\n",
    "        \n",
    "    def ngram_weight(self):\n",
    "        \"\"\"\n",
    "        This function culculates significance of the entire ngram\n",
    "        based on the significance of its parts of speech\n",
    "        \"\"\"\n",
    "        s_pos = [self.g.POS_values[wp] * self.pos[wp] for wp in self.pos]\n",
    "        s_pos = sum(s_pos) / self.n_words\n",
    "        if len(pos) == 1: \n",
    "            s_pos *= 0.8\n",
    "        return s_pos\n",
    "    \n",
    "    def __check_ldeps(self, dep, found):\n",
    "        \"\"\"\n",
    "        This function checks compound dependencies\n",
    "        \"\"\"\n",
    "        for d in dep:\n",
    "            if d in self.gram:\n",
    "                self.gram.remove(d)\n",
    "                found += 1\n",
    "                break\n",
    "        return found\n",
    "    \n",
    "    def __check_dependencies(self):\n",
    "        \"\"\"\n",
    "        This function checks dependencies in a ngram\n",
    "        \"\"\"\n",
    "        found = 0\n",
    "        for dep in self.deps:\n",
    "            if isinstance(dep, list):\n",
    "                found = self.__check_ldeps(dep, found)\n",
    "            elif dep in self.gram:\n",
    "                self.gram.remove(dep)\n",
    "                found += 1\n",
    "        if found != len(self.deps):\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def ifngram(self):\n",
    "        \"\"\"\n",
    "        This function checks if the word sequence is an ngram (=construction)\n",
    "        \"\"\"\n",
    "        if 'V' in self.pos and self.n_words > 2: \n",
    "            dep = self.__check_dependencies()\n",
    "            val = self.ngram_weight()\n",
    "            if dep and val > 0.9: return True\n",
    "        return False\n",
    "        \n",
    "    def clean_adj(self):\n",
    "        \"\"\"\n",
    "        This function shortens the sequence of adjectives\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        adj = False\n",
    "        for w in self.result:\n",
    "            if adj and w.pos != 'Adj': adj = False\n",
    "            if not adj: res.append(w)\n",
    "            if w.pos == 'Adj': adj = True\n",
    "        res = sorted(res)\n",
    "        self.result = tuple(res)\n",
    "        \n",
    "    def add_ngram(self, all_ngrams):\n",
    "        \"\"\"\n",
    "        This function adds ngram to final list of ngrams\n",
    "        \"\"\"\n",
    "        if self.ifngram():\n",
    "            self.clean_adj()\n",
    "            all_ngrams.add(self.result)\n",
    "            self.reboot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_maker(sent, start, finish, all_ngrams):\n",
    "    \n",
    "    if len(sent) < 4: start = len(sent)\n",
    "    if len(sent) < 8: finish = len(sent)\n",
    "        \n",
    "    for num_w in range(start, finish):\n",
    "        for ng in ngrams(sent, num_w):\n",
    "            storage = Ngram_storage()\n",
    "            \n",
    "            for index, w in enumerate(ng):\n",
    "                if w.lemma in skip_conj:\n",
    "                    if index == 0 or index == len(ng)-1:\n",
    "                        continue\n",
    "#                 elif w.pos == 'pun' and w.lemma != ',':\n",
    "#                     storage.add_ngram(all_ngrams)\n",
    "                elif w.pos != 'pun': \n",
    "                    storage.add_word(w)\n",
    "            if storage.result: \n",
    "                storage.add_ngram(all_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f590ea3cac44aa089e4c01e3263f124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=101359.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_ngrams = AllNgrams()\n",
    "start, finish = 4, 8\n",
    "skip_conj = ('og', 'en', 'eða')\n",
    "\n",
    "for sent in tqdm(sentences):\n",
    "    ngram_maker(sent, start, finish, all_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "663775"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_ngrams.freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате получилось 663775 конструкций разной степени вариативности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrams_sort = sorted(all_ngrams.freq.items(), key=lambda kv: kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((PProNom, PrpNom, V_svarar), 378),\n",
       " ((C_er, NNom_maður, PrpNom, V_hét), 386),\n",
       " ((C_er, PProNom, V_komu), 414),\n",
       " ((PProDat, PrpNom, V_segir), 416),\n",
       " ((PProNom, PrpNom, V_mælti), 431),\n",
       " ((C_að, PProNom, PrpNom, V_segir), 441),\n",
       " ((PProNom, PProNom, V_segir), 481),\n",
       " ((C_er, PrpNom, V_hét), 580),\n",
       " ((C_að, PProNom, V_segir), 587),\n",
       " ((C_er, PrpNom, V_átti), 594),\n",
       " ((C_að, PrpNom, V_segir), 676),\n",
       " ((NNom_son, PProGen, PrpNom, V_var), 677),\n",
       " ((C_er, PrpNom, PrpNom, V_átti), 731),\n",
       " ((PProNom, PrpNom, V_segir), 860),\n",
       " ((ADV_þá, PrpNom, V_mælti), 1637)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrams_sort[-15:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Фильтрация на основе частотности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как в ходе формирования нграмм рассматривались сразу несколько размеров окон, то часть нграмм представляется собой дублирование одного и того же предложения. Например, выражение <i>Og er Ásbirni komu orð bróður síns</i> может быть представлено в данных 4 раза:\n",
    "\n",
    "    1) Og er Ásbirni komu - окно в 4 слова \n",
    "    2) Og er Ásbirni komu orð - окно в 5 слов\n",
    "    3) Og er Ásbirni komu orð bróður - окно в 6 слов\n",
    "    4) Og er Ásbirni komu orð bróður síns - окно в 7 слов\n",
    "\n",
    "Для того, чтобы сократить подобное дублирование, все конструкции были сгруппированы по частотности. Если в одну группу попали конструкции, отличающиеся лишь словами в конце конструкции, значит, это варианты одной длинной конструкции, которая была продублирована окнами разной длины. Группировка по частотности позволяет избежать сокращения конструкций, встручающихся и в других предложениях. Так, если <i>Og er Ásbirni komu</i> встретиться сразу в нескольких предложениях, ее частотность будет выше, чем у остальных вариантов. Это отличие позволяет рассматривать <i>Og er Ásbirni komu</i> уже не как часть большей конструкции, но как самостоятельную единицу. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_variants(freq_grouped):\n",
    "    unique = []\n",
    "    ind, lead = 0, ''\n",
    "    for group in freq_grouped:\n",
    "        for index, item in enumerate(freq_grouped[group]):\n",
    "            if index == 0 or item[:ind] == lead: \n",
    "                lead = item\n",
    "                ind = len(item)\n",
    "            elif item[:ind] != lead:\n",
    "                unique.append(lead)\n",
    "                lead = item\n",
    "                ind = len(item) \n",
    "    return list(reversed(unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_grouped = defaultdict(list) # частотность: нграммы\n",
    "\n",
    "for key, value in nrams_sort:\n",
    "    freq_grouped[value].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freq_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ngrams = filter_variants(freq_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того, как конструкции были отфильтрованы, они были записаны в базу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT representation, id FROM ngram_item\")\n",
    "token_mapping = dict(cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_to_db = []\n",
    "content_to_db = []\n",
    "coords_to_db = []\n",
    "for idx, ng in enumerate(unique_ngrams):\n",
    "    ngram_to_db.append((idx, all_ngrams.freq[ng]))\n",
    "    for word in ng:\n",
    "        content_to_db.append((idx, token_mapping[word.show]))\n",
    "    for coord in all_ngrams.coord[ng]:\n",
    "        coords_to_db.append((idx, coord.text_index, coord.sent_index, coord.word_start, coord.word_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.executemany(\"INSERT INTO ngram_coords (unique_ngram_id, text_id, sentence_id, start, end) VALUES (?, ?, ?, ?, ?)\", coords_to_db)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.executemany(\"INSERT INTO ngram_content (ngram_id, ngram_item_id) VALUES (?, ?)\", content_to_db)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.executemany(\"INSERT INTO unique_ngrams (id, n_occurrences) VALUES (?, ?)\", ngram_to_db)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Упрощение нграмм\n",
    "\n",
    "После первичной фильтрации и схлопывания, было произведено упрощение нграмм, которое заключалось в следующих этапах:\n",
    "\n",
    "    1) формирование \"коротких представления\" нграмм (нграммы с опущенными прилагательными и наречиями)\n",
    "    2) формирование \"синтаксического представления\" нграмм за счет опущения лемм\n",
    "    \n",
    "Этот этап необходим для дальнейшей кластеризации. На основе получившихся упрощенных представлений нграммы группировались в наборы, в рамках которых производилась кластеризация, что позволяет находить расстояния только между схожими синтаксическими конструкциями.  \n",
    "\n",
    "\n",
    "short_ngram - это без прилагательных и наречий\n",
    "\n",
    "pos_ngrams - это только с метками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT id FROM ngram_item WHERE pos NOT IN ('', 'ADV', 'Adj')\")\n",
    "good = cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создать индекс по ngram_item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"\n",
    "CREATE INDEX `ngram_item_id_index` ON `ngram_content` (\n",
    "\t`ngram_item_id`\tASC\n",
    ");\n",
    "\"\"\")\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.executemany(\"UPDATE ngram_content SET keep = 1 WHERE ngram_item_id = ?\", good)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"UPDATE ngram_content SET keep = 0 WHERE keep IS NULL\")\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2470964.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cur.execute(\"\"\"\n",
    "SELECT ngram_id, kind\n",
    "FROM ngram_content\n",
    "    JOIN ngram_item ON ngram_content.ngram_item_id = ngram_item.id\n",
    "WHERE keep = 1\n",
    "ORDER BY ngram_id, kind\"\"\")\n",
    "data = cur.fetchall()\n",
    "\n",
    "ngram_dict = {}\n",
    "short_ngram_list = []\n",
    "ngram_update = []\n",
    "ngram_idx = 0\n",
    "\n",
    "current_ngram = tuple()\n",
    "current = 0\n",
    "for row in tqdm(data, leave=False):\n",
    "    if row[0] != current:\n",
    "        if current_ngram:\n",
    "            if current_ngram not in ngram_dict:\n",
    "                ngram_dict[current_ngram] = ngram_idx\n",
    "                short_ngram_list.append((ngram_idx, ';'.join(current_ngram)))\n",
    "                ngram_idx += 1\n",
    "            ngram_update.append((ngram_dict[current_ngram], current))\n",
    "            current = row[0]\n",
    "            current_ngram = (row[1],)\n",
    "    else:\n",
    "        current_ngram += (row[1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NNom', 'PProNom', 'PrpNom', 'V')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12747, 12747, 658132)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngram_dict), len(short_ngram_list), len(ngram_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.executemany(\"UPDATE unique_ngrams SET short_ngram_id = ? WHERE id = ?\", ngram_update)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'C;PProNom;PProNom;V'),\n",
       " (1, 'C;PProDat;PProNom;V'),\n",
       " (2, 'PProNom;V'),\n",
       " (3, 'NNom;PProGen;PrpNom;V'),\n",
       " (4, 'C;V;V')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_ngram_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.executemany(\"INSERT INTO short_ngrams (id, representation) VALUES (?, ?)\", short_ngram_list)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT * \n",
    "FROM ngram_content\n",
    "\tJOIN ngram_item ON ngram_content.ngram_item_id = ngram_item.id\n",
    "\tJOIN unique_ngrams ON ngram_content.ngram_id = unique_ngrams.id\n",
    "WHERE short_ngram_id = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"\n",
    "SELECT count(ngram_coords.id), unique_ngrams.short_ngram_id \n",
    "FROM ngram_coords\n",
    "\tJOIN unique_ngrams ON ngram_coords.unique_ngram_id = unique_ngrams.id\n",
    "WHERE unique_ngrams.short_ngram_id IS NOT NULL\n",
    "GROUP BY unique_ngrams.short_ngram_id\"\"\")\n",
    "data = cur.fetchall()\n",
    "\n",
    "cur.executemany(\"UPDATE short_ngrams SET n_occurrences = ? WHERE id = ?\", data)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Кластеризация\n",
    "\n",
    "Выбираем 1 pos_ngram\n",
    "\n",
    "Подтягиваем нграм-контент keep\n",
    "\n",
    "Оправляем в кластеризацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clusters:\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "#         self.load_model(model_path)\n",
    "        self.coefs = {'V': 3, 'PREP': 2, 'PProNom': 1.5, 'Inf': 1.5, 'C': 2}\n",
    "        \n",
    "    def load_model(self, model_path):\n",
    "        \"\"\"\n",
    "        Load model\n",
    "        \"\"\"\n",
    "        self.model = load_facebook_vectors(model_path)\n",
    "    \n",
    "    def __parse_single(self, w_list, coef):\n",
    "        \"\"\"\n",
    "        Compute similarities\n",
    "        \"\"\"\n",
    "        return abs(1 - cosine_similarity(self.model[w_list])) * coef\n",
    "    \n",
    "    def __parse_double(self, w_list1, w_list2, coef):\n",
    "        \"\"\"\n",
    "        Align double pos-tags and compute similarities\n",
    "        \"\"\"\n",
    "        w_list1 = self.model[w_list1]\n",
    "        w_list2 = self.model[w_list2]\n",
    "\n",
    "        A = cosine_similarity(w_list1)\n",
    "        B = cosine_similarity(w_list2)\n",
    "        C = cosine_similarity(w_list1, w_list2)\n",
    "        D = C.T\n",
    "\n",
    "        Z = np.concatenate([A.flatten(), B.flatten(), C.flatten(), D.flatten()])\n",
    "        Z = Z.reshape((4, A.flatten().shape[0]))\n",
    "        M = Z.argmax(axis=0)\n",
    "\n",
    "        R = ((Z[0:2,] * (M <= 1)) + (Z[2:4,] * (M > 1))).sum(axis=0).reshape(A.shape)\n",
    "        return abs(2 - R) * coef\n",
    "    \n",
    "    @staticmethod\n",
    "    def __get_sentences(short_id):\n",
    "        \"\"\"\n",
    "        Select ngram content for chosen type\n",
    "        \"\"\"\n",
    "        cur.execute(\"\"\"\n",
    "        SELECT ngram_id, lemma, kind \n",
    "        FROM ngram_content\n",
    "            JOIN ngram_item ON ngram_content.ngram_item_id = ngram_item.id\n",
    "            JOIN unique_ngrams ON ngram_content.ngram_id = unique_ngrams.id\n",
    "        WHERE short_ngram_id = ? AND keep = 1\n",
    "        \"\"\", (short_id,))\n",
    "        return cur.fetchall()\n",
    "    \n",
    "    @staticmethod\n",
    "    def __df_for_unique(data):\n",
    "        \"\"\"\n",
    "        Create df for unique structure (C, V)\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(data, columns=[\"ngram_id\", \"lemma\", \"kind\"])\n",
    "        df[\"lemma\"] = df[[\"lemma\", \"kind\"]].apply(lambda x: x[\"lemma\"] or x[\"kind\"], axis=1)\n",
    "        df = df.pivot_table(index=\"ngram_id\", columns=\"kind\", values=\"lemma\", aggfunc='first')\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def __df_for_non_unique(phrases_list):\n",
    "        \"\"\"\n",
    "        Create df for non-unique structure (C, V, V)\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        idxs = []\n",
    "        \n",
    "        current_ngram = phrases_list[0][0]\n",
    "        current_row = {}\n",
    "        \n",
    "        for idx, row in enumerate(phrases_list):\n",
    "            ng_id, lemma, kind = row\n",
    "            lemma_to_write = lemma or kind\n",
    "            \n",
    "            if ng_id != current_ngram:\n",
    "                data.append(current_row)\n",
    "                idxs.append(current_ngram)\n",
    "                current_row = {}\n",
    "                current_ngram = ng_id\n",
    "            \n",
    "            if kind in current_row:\n",
    "                current_row[f\"{kind}_2\"] = lemma_to_write\n",
    "            else:\n",
    "                current_row[kind] = lemma_to_write\n",
    "        data.append(current_row)\n",
    "        idxs.append(current_ngram)\n",
    "        df = pd.DataFrame(data)\n",
    "        cols = df.columns.tolist()\n",
    "        df.index = idxs\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def __make_dataframe(self, short_id, key):\n",
    "        \"\"\"\n",
    "        Create dataframe depending on ngram type\n",
    "        \"\"\"\n",
    "        data = self.__get_sentences(short_id)\n",
    "        if max(Counter(key.split(\";\")).values()) == 1:\n",
    "            df = self.__df_for_unique(data)\n",
    "        else:\n",
    "            df = self.__df_for_non_unique(data)\n",
    "        return df\n",
    "        \n",
    "    \n",
    "    def __get_distance_matrix(self, key, df):\n",
    "        \"\"\"\n",
    "        Compute distance matrix\n",
    "        \"\"\"\n",
    "        poss = Counter(key.split(\";\"))\n",
    "        matrix = np.zeros((df.shape[0], df.shape[0]))\n",
    "        for pos in poss:\n",
    "            coef = self.coefs.get(pos, 1)\n",
    "            if poss[pos] == 2:\n",
    "                matrix = np.add(matrix, self.__parse_double(df[pos].values, df[f\"{pos}_2\"].values, coef=coef), out=matrix)\n",
    "            elif poss[pos] == 1:\n",
    "                matrix = np.add(matrix, self.__parse_single(df[pos].values, coef=coef), out=matrix)\n",
    "        matrix = matrix / len(poss)\n",
    "        return matrix\n",
    "    \n",
    "    @staticmethod\n",
    "    def __validate_key(representation):\n",
    "        \"\"\"\n",
    "        Validate ngram type: max 2 same pos-tags\n",
    "        \"\"\"\n",
    "        return max(Counter(representation.split(\";\")).values()) <= 2\n",
    "    \n",
    "    def get_keys(self):\n",
    "        \"\"\"\n",
    "        Select all valid ngram types\n",
    "        \"\"\"\n",
    "        cur.execute(\"\"\"\n",
    "        SELECT id, representation FROM short_ngrams\n",
    "        WHERE n_occurrences > 10\"\"\")\n",
    "        data = [i for i in cur.fetchall() if self.__validate_key(i[1])]\n",
    "        return data\n",
    "        \n",
    "    def __run_one(self, short_id, key):\n",
    "        \"\"\"\n",
    "        Run clusterization for one ngram type\n",
    "        \"\"\"\n",
    "        df = self.__make_dataframe(short_id, key)\n",
    "        \n",
    "        M = self.__get_distance_matrix(key, df)\n",
    "        Z = complete(M)\n",
    "        C = fcluster(Z, t=1, criterion=\"distance\")\n",
    "        \n",
    "        cntr = {k: v for k, v in Counter(C).items() if v > 1} \n",
    "        cluster_result = pd.DataFrame({\"ids\": df.index, \"clusters\": C, \"short_id\": short_id})\n",
    "        cluster_result = cluster_result[cluster_result[\"clusters\"].isin(cntr)]\n",
    "        \n",
    "        cur.executemany(\n",
    "            \"\"\"INSERT INTO clustering (ngram_id, short_ngram_id, cluster_idx)VALUES (?, ?, ?)\"\"\", \n",
    "            cluster_result[[\"ids\", \"short_id\", \"clusters\"]].values.tolist())\n",
    "        db.commit()\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run clusterization on all ngrams\n",
    "        \"\"\"\n",
    "        keys = self.get_keys()  #[1000:]  # - первые тяжелые, проще тестить на поменьше\n",
    "        for idx, short_form in tqdm(keys, leave=False):\n",
    "            self.__run_one(idx, short_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# тут не через класс загрузка модели, потому что так проще тестировать, а то модель загружается долго и тяжело\n",
    "\n",
    "model_path = '/home/dkbrz/data/cc.is.300.bin'\n",
    "\n",
    "model = load_facebook_vectors(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = Clusters(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ff061413864489a7e643b589a6ccee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2790.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clusters.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
